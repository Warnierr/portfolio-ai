export interface BlogPost {
  slug: string;
  title: string;
  excerpt: string;
  content: string;
  date: string;
  readTime: string;
  category: 'spark' | 'airflow' | 'dataops' | 'ai';
  tags: string[];
  author: string;
}

export const blogPosts: BlogPost[] = [
  {
    slug: "5-erreurs-spark-production",
    title: "5 erreurs courantes avec Apache Spark en production",
    excerpt: "Les pi√®ges classiques qui font crasher vos jobs Spark et comment les √©viter. Retour d'exp√©rience sur des pipelines traitant des TBs de donn√©es.",
    date: "5 janvier 2026",
    readTime: "8 min",
    category: "spark",
    tags: ["spark", "scala", "pyspark", "optimisation", "production"],
    author: "Raouf Warnier",
    content: `# 5 erreurs courantes avec Apache Spark en production

Apr√®s 7 ans √† d√©velopper et maintenir des pipelines Spark en production (BNP Paribas, Orange, ACC Industrie 4.0), j'ai identifi√© 5 erreurs qui reviennent syst√©matiquement et qui peuvent faire crasher vos jobs ou exploser vos co√ªts.

## 1. Mauvais partitioning des donn√©es

### Le probl√®me
Un partitioning inadapt√© cr√©e des **skewed partitions** : certaines partitions contiennent 100x plus de donn√©es que d'autres. R√©sultat : un seul executor travaille pendant que les autres attendent.

### Sympt√¥mes
- Job qui stagne √† 99% pendant des heures
- OOM (Out of Memory) sur certains executors
- Temps d'ex√©cution impr√©visible

### Solution
\`\`\`scala
// ‚ùå Mauvais : partitioning par d√©faut
df.groupBy("user_id").agg(...)

// ‚úÖ Bon : repartition avant aggregation
df.repartition(200, col("user_id"))
  .groupBy("user_id")
  .agg(...)
\`\`\`

**R√®gle empirique** : 1 partition = 128 MB de donn√©es. Pour 100 GB ‚Üí ~800 partitions.

## 2. Broadcast joins sur des tables trop grandes

### Le probl√®me
Spark peut broadcaster une petite table (< 10 MB par d√©faut) vers tous les executors pour √©viter un shuffle. Mais si vous forcez le broadcast d'une table de 500 MB, **tous les executors crashent avec OOM**.

### Solution
\`\`\`scala
// ‚ùå Mauvais : forcer broadcast sur grosse table
import org.apache.spark.sql.functions.broadcast
df1.join(broadcast(df2), "key")

// ‚úÖ Bon : laisser Spark d√©cider OU augmenter threshold
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "50MB")
df1.join(df2, "key")
\`\`\`

**V√©rifiez** : \`df.count()\` et \`df.cache().count()\` pour estimer la taille r√©elle.

## 3. Pas de cache sur les DataFrames r√©utilis√©s

### Le probl√®me
Si vous utilisez le m√™me DataFrame dans plusieurs transformations sans le cacher, **Spark recalcule tout depuis le d√©but √† chaque fois**.

### Solution
\`\`\`scala
// ‚ùå Mauvais : recalcul √† chaque action
val df = spark.read.parquet("data.parquet")
df.filter(...).count()  // Lit tout
df.filter(...).show()   // Relit tout !

// ‚úÖ Bon : cache apr√®s premi√®re lecture
val df = spark.read.parquet("data.parquet").cache()
df.count() // Force le cache
df.filter(...).count()
df.filter(...).show()
\`\`\`

**Attention** : \`unpersist()\` apr√®s usage pour lib√©rer la m√©moire !

## 4. Trop de petits fichiers (Small Files Problem)

### Le probl√®me
√âcrire 10 000 fichiers de 1 MB au lieu de 100 fichiers de 100 MB **d√©grade les performances de 10x** lors des lectures futures.

### Solution
\`\`\`scala
// ‚ùå Mauvais : 1 fichier par partition
df.write.parquet("output/")

// ‚úÖ Bon : coalesce avant √©criture
df.coalesce(100).write.parquet("output/")

// ‚úÖ Encore mieux : repartition par cl√© m√©tier
df.repartition(100, col("date"))
  .write
  .partitionBy("date")
  .parquet("output/")
\`\`\`

**R√®gle** : Viser 128-256 MB par fichier.

## 5. Ignorer les m√©triques Spark UI

### Le probl√®me
Beaucoup de d√©veloppeurs lancent un job et attendent sans regarder **Spark UI**. Pourtant, c'est l√† que vous voyez :
- Les stages qui prennent 90% du temps
- Les shuffle read/write excessifs
- Les GC (Garbage Collection) qui tuent les perfs

### Solution
1. Ouvrir **Spark UI** : \`http://localhost:4040\` (ou via YARN/K8s)
2. Onglet **Stages** : identifier les stages lents
3. Onglet **Executors** : v√©rifier la distribution des t√¢ches
4. Onglet **SQL** : voir le plan d'ex√©cution

**Exemple concret** : J'ai r√©duit un job de 4h √† 30 min en d√©tectant via Spark UI qu'un \`groupBy\` cr√©ait un shuffle de 500 GB. Solution : \`repartition\` avant le \`groupBy\`.

## Bonus : Tuning m√©moire

### Configuration typique pour jobs stables
\`\`\`bash
spark-submit \\
  --executor-memory 8G \\
  --executor-cores 4 \\
  --driver-memory 4G \\
  --conf spark.executor.memoryOverhead=2G \\
  --conf spark.sql.shuffle.partitions=200 \\
  my_job.py
\`\`\`

**R√®gle** : \`memoryOverhead = 10-20% de executor-memory\` pour √©viter les OOM.

## Conclusion

Ces 5 erreurs repr√©sentent **80% des probl√®mes Spark en production** que j'ai rencontr√©s. Les corriger peut diviser vos temps d'ex√©cution par 3-5 et vos co√ªts par 2.

**Prochaines √©tapes** :
1. Auditer vos jobs actuels avec Spark UI
2. V√©rifier le partitioning de vos datasets critiques
3. Impl√©menter du caching strat√©gique

Besoin d'aide pour optimiser vos pipelines Spark ? [Contactez-moi](/contact) pour un diagnostic gratuit.

---

**Tags** : #spark #scala #pyspark #optimisation #production #dataengineering
`,
  },
  {
    slug: "airflow-patterns-anti-fragiles",
    title: "Airflow : patterns anti-fragiles pour pipelines robustes",
    excerpt: "Comment construire des DAGs Airflow qui ne cassent jamais en production. Idempotence, retry intelligents et monitoring proactif.",
    date: "5 janvier 2026",
    readTime: "10 min",
    category: "airflow",
    tags: ["airflow", "dataops", "orchestration", "monitoring", "production"],
    author: "Raouf Warnier",
    content: `# Airflow : patterns anti-fragiles pour pipelines robustes

Apr√®s avoir d√©ploy√© des dizaines de DAGs Airflow en production chez Orange et BNP Paribas, j'ai appris une chose : **un pipeline qui ne peut pas √™tre relanc√© sans effets de bord est un pipeline fragile**.

Voici les patterns que j'applique syst√©matiquement pour des DAGs anti-fragiles.

## 1. Idempotence : le principe fondamental

### D√©finition
Un DAG est **idempotent** si vous pouvez le relancer 10 fois sur la m√™me p√©riode sans casser les donn√©es.

### Exemple concret
\`\`\`python
# ‚ùå Mauvais : append sans v√©rification
def load_data(**context):
    df = extract_data()
    df.to_sql('table', engine, if_exists='append')  # Risque de doublons !

# ‚úÖ Bon : upsert avec cl√© unique
def load_data(**context):
    execution_date = context['execution_date']
    df = extract_data()
    
    # Supprimer les donn√©es existantes pour cette date
    engine.execute(f"DELETE FROM table WHERE date = '{execution_date}'")
    
    # Ins√©rer les nouvelles donn√©es
    df.to_sql('table', engine, if_exists='append')
\`\`\`

**Pourquoi c'est crucial** : En prod, les DAGs crashent (API timeout, OOM, etc.). Si vous ne pouvez pas les relancer sans cr√©er des doublons, vous √™tes bloqu√©.

## 2. Retry intelligents (pas infinis)

### Le probl√®me
Par d√©faut, Airflow retry 0 fois. Mais mettre \`retries=999\` n'est pas la solution : si une API est down 24h, votre DAG va retry toutes les 5 minutes pendant 24h !

### Solution : retry progressif
\`\`\`python
from airflow.operators.python import PythonOperator
from datetime import timedelta

task = PythonOperator(
    task_id='extract_api',
    python_callable=extract_from_api,
    retries=3,  # 3 tentatives max
    retry_delay=timedelta(minutes=5),  # Attendre 5 min entre chaque
    retry_exponential_backoff=True,  # 5min, 10min, 20min
    max_retry_delay=timedelta(hours=1),  # Cap √† 1h max
)
\`\`\`

**R√®gle** : 
- API externes : \`retries=3\`, \`retry_delay=5min\`, exponential backoff
- Jobs internes (Spark) : \`retries=1\` (si √ßa crash, c'est un vrai bug)

## 3. Monitoring proactif avec alerting

### Le probl√®me classique
Votre DAG fail √† 3h du matin. Personne ne le voit. Les utilisateurs d√©couvrent √† 9h que les donn√©es sont manquantes. **Trop tard.**

### Solution : Alerting Telegram/Slack
\`\`\`python
from airflow.providers.telegram.hooks.telegram import TelegramHook

def send_telegram_alert(context):
    task_instance = context['task_instance']
    dag_id = context['dag'].dag_id
    
    message = f"""
    üö® Airflow Alert
    
    DAG: {dag_id}
    Task: {task_instance.task_id}
    √âtat: FAILED
    Logs: {task_instance.log_url}
    """
    
    hook = TelegramHook(telegram_conn_id='telegram_default')
    hook.send_message({'text': message, 'chat_id': 'YOUR_CHAT_ID'})

# Appliquer √† tous les DAGs
default_args = {
    'on_failure_callback': send_telegram_alert,
    'on_retry_callback': send_telegram_alert,
}
\`\`\`

**Bonus** : Int√©grer avec Grafana/Prometheus pour dashboards temps r√©el.

## 4. Gestion des d√©pendances externes

### Le probl√®me
Votre DAG d√©pend d'un fichier S3 qui arrive entre 2h et 4h du matin. Si vous lancez √† 3h fixe, √ßa marche 50% du temps.

### Solution : Sensors
\`\`\`python
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor

wait_for_file = S3KeySensor(
    task_id='wait_for_data',
    bucket_name='my-bucket',
    bucket_key='data/{{ ds }}/file.csv',
    timeout=3600,  # Attendre max 1h
    poke_interval=300,  # V√©rifier toutes les 5 min
    mode='reschedule',  # Lib√©rer le worker entre chaque check
)

process_data = PythonOperator(...)

wait_for_file >> process_data
\`\`\`

**Modes** :
- \`poke\` : bloque le worker (pour checks rapides < 5 min)
- \`reschedule\` : lib√®re le worker (pour checks longs)

## 5. Validation des donn√©es (Data Quality)

### Le probl√®me
Votre pipeline s'ex√©cute avec succ√®s, mais les donn√©es sont pourries (colonnes nulles, doublons, valeurs aberrantes). Personne ne le voit avant que les dashboards soient faux.

### Solution : Great Expectations + Airflow
\`\`\`python
from airflow.operators.python import PythonOperator
import great_expectations as ge

def validate_data(**context):
    df = pd.read_sql("SELECT * FROM table WHERE date = '{{ ds }}'", engine)
    
    # Convertir en GE dataset
    ge_df = ge.from_pandas(df)
    
    # D√©finir les attentes
    ge_df.expect_column_values_to_not_be_null('user_id')
    ge_df.expect_column_values_to_be_unique('transaction_id')
    ge_df.expect_column_values_to_be_between('amount', min_value=0, max_value=1000000)
    
    # Valider
    results = ge_df.validate()
    
    if not results['success']:
        raise ValueError(f"Data quality check failed: {results}")

validate = PythonOperator(
    task_id='validate_data',
    python_callable=validate_data,
)

extract >> transform >> load >> validate  # Validation APR√àS le load
\`\`\`

**Principe** : Fail fast si les donn√©es sont mauvaises.

## 6. Templating pour √©viter le code dupliqu√©

### Le probl√®me
Vous avez 10 DAGs quasi identiques qui diff√®rent juste par le nom de la table. Maintenir 10 fichiers Python est l'enfer.

### Solution : DAG Factory
\`\`\`python
from airflow import DAG
from datetime import datetime

def create_etl_dag(table_name, schedule):
    dag = DAG(
        dag_id=f'etl_{table_name}',
        schedule_interval=schedule,
        start_date=datetime(2026, 1, 1),
        catchup=False,
    )
    
    with dag:
        extract = PythonOperator(
            task_id='extract',
            python_callable=extract_data,
            op_kwargs={'table': table_name},
        )
        
        load = PythonOperator(
            task_id='load',
            python_callable=load_data,
            op_kwargs={'table': table_name},
        )
        
        extract >> load
    
    return dag

# G√©n√©rer 10 DAGs automatiquement
tables = ['users', 'orders', 'products', 'transactions']
for table in tables:
    globals()[f'etl_{table}'] = create_etl_dag(table, '@daily')
\`\`\`

**R√©sultat** : 1 seul fichier Python pour 10 DAGs.

## 7. Backfill s√©curis√©

### Le probl√®me
Vous devez retraiter 6 mois de donn√©es. Si vous lancez un backfill na√Øf, vous allez :
- Saturer votre cluster
- Crasher vos bases de donn√©es
- Bloquer les DAGs en cours

### Solution : Backfill contr√¥l√©
\`\`\`bash
# ‚ùå Mauvais : backfill tout d'un coup
airflow dags backfill my_dag --start-date 2025-07-01 --end-date 2026-01-01

# ‚úÖ Bon : backfill par batch avec delay
for month in {07..12}; do
    airflow dags backfill my_dag \\
        --start-date 2025-$month-01 \\
        --end-date 2025-$month-31 \\
        --delay-on-limit 60  # Attendre 60s entre chaque run
    sleep 300  # Pause de 5 min entre chaque mois
done
\`\`\`

**Alternative** : Cr√©er un DAG de backfill d√©di√© avec \`max_active_runs=1\`.

## Conclusion

Ces 7 patterns transforment des DAGs fragiles en pipelines anti-fragiles :
1. ‚úÖ **Idempotence** : relan√ßable sans risque
2. ‚úÖ **Retry intelligents** : pas infinis
3. ‚úÖ **Alerting proactif** : d√©tection imm√©diate
4. ‚úÖ **Sensors** : attendre les d√©pendances
5. ‚úÖ **Data quality** : validation automatique
6. ‚úÖ **Templating** : √©viter la duplication
7. ‚úÖ **Backfill contr√¥l√©** : retraitement s√©curis√©

**R√©sultat** : Pipelines qui tournent en prod sans intervention humaine pendant des mois.

Besoin d'aide pour industrialiser vos DAGs Airflow ? [Contactez-moi](/contact).

---

**Tags** : #airflow #dataops #orchestration #monitoring #production
`,
  },
];
